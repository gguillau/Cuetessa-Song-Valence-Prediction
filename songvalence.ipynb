{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d976c6",
   "metadata": {},
   "source": [
    "# Task for Cuetessa, Inc. – Predicting Valence of Pop Songs\n",
    "\n",
    "## Overview\n",
    "The aim of this task is to develop a Python-based module to predict the valence of newly released pop songs.  Two approaches are to use as input \n",
    "1. the audio data (e.g., .wav files) of songs  \n",
    "2. the lyrics of songs.  Publicly available datasets can be used for training and testing. \n",
    "\n",
    "\n",
    "### Data Description \n",
    "\n",
    "DEAM dataset (DEAM dataset - The MediaEval Database for Emotional Analysis of Music) consists of 1802 excerpts and full songs annotated with valence and arousal values both continuously (per-second) and over the whole song. The metadata describing the audio excerpts (their duration, genre, folksonomy tags).\n",
    "\n",
    "- Annotations Data: The annotated dataset comes from Soleymani et al. (2013) (http://cvml.unige.ch/databases/emoMusic/). It consists of 45-s clips of 744 songs from the Free Music Archive (https://freemusicarchive.org/) that span a variety of popular genres\n",
    "    - Annotations are made available in csv format. There are six csv files in this database, four containing\n",
    "average and standard deviation of arousal and valence continuous annotation for each song.\n",
    "- Metadata: \n",
    "    - including, song title, genre and artist is also provided.\n",
    "    \n",
    "    \n",
    "### Feature Extraction\n",
    "In this version of the project, the approach of the following research paper will be implemented: [Measuring national mood with music](https://link.springer.com/article/10.3758/s13428-021-01747-7). Specifically, the same [set of features](https://static-content.springer.com/esm/art%3A10.3758%2Fs13428-021-01747-7/MediaObjects/13428_2021_1747_MOESM1_ESM.pdf) will be tried to extract. These features are:\n",
    "- [x] Spectral Centroid;\n",
    "- [x] Spectral Rolloff;\n",
    "- [x] Spectral Contrast — ~~7 bands~~ *(kept the default 6 bands instead)*;\n",
    "- [x] Mel-Frequency Cepstrum Coefficients (MFCC) — 24 coefficients;\n",
    "- [x] Zero Crossing Rate;\n",
    "- [x] Chroma Energy Normalized Statistics (CENS) — 12 chroma;\n",
    "- [x] Beat Per Minute (BPM);\n",
    "- [x] Root Mean Square (RMS);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea8ad8",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# statistical visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa as librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "from scipy import stats as st\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# import module for splitting and cross-validation using gridsearch\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import machine learning module from the sklearn library\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from datetime import date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9eecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some parameters for plots in this notebook\n",
    "plt.style.use(\"seaborn-paper\")\n",
    "sns.set_style(\"white\")\n",
    "sns.set_palette(\"flare\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95da637",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine if columns in file have null values\n",
    "def get_percent_of_na(df, num):\n",
    "    count = 0\n",
    "    df = df.copy()\n",
    "    s = (df.isna().sum() / df.shape[0])\n",
    "    for column, percent in zip(s.index, s.values):\n",
    "        num_of_nulls = df[column].isna().sum()\n",
    "        if num_of_nulls == 0:\n",
    "            continue\n",
    "        else:\n",
    "            count += 1\n",
    "        print('Column {} has {:.{}%} percent of Nulls, and {} of nulls'.format(column, percent, num, num_of_nulls))\n",
    "    if count != 0:\n",
    "        print(\"\\033[1m\" + 'There are {} columns with NA.'.format(count) + \"\\033[0m\")\n",
    "    else:\n",
    "        print()\n",
    "        print(\"\\033[1m\" + 'There are no columns with NA.' + \"\\033[0m\")\n",
    "        \n",
    "# function to display general information about the dataset\n",
    "def get_info(df):\n",
    "    \"\"\"\n",
    "    This function uses the head(), info(), describe(), shape() and duplicated() \n",
    "    methods to display the general information about the dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\033[1m\" + '-'*100 + \"\\033[0m\")\n",
    "    print('Head:')\n",
    "    print()\n",
    "    display(df.head())\n",
    "    print('-'*100)\n",
    "    print('Info:')\n",
    "    print()\n",
    "    display(df.info())\n",
    "    print('-'*100)\n",
    "    print('Describe:')\n",
    "    print()\n",
    "    for column in df:\n",
    "        if df[column].dtype == 'object':\n",
    "            display(df.describe(include='object'))\n",
    "        else:\n",
    "            display(df.describe())\n",
    "    print('-'*100)\n",
    "    print()\n",
    "    print('Columns with nulls:')\n",
    "    display(get_percent_of_na(df, 4))  # check this out\n",
    "    print('-'*100)\n",
    "    print('Shape:')\n",
    "    print(df.shape)\n",
    "    print('-'*100)\n",
    "    print('Duplicated:')\n",
    "    print(\"\\033[1m\" + 'We have {} duplicated rows.\\n'.format(df.duplicated().sum()) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06259051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "try:\n",
    "    annotations = pd.read_csv('/Users/gguillau/Desktop/Practicum/Cuetessa Project/archive/DEAM_Annotations/annotations/annotations averaged per song/song_level/static_annotations_averaged_songs_1_2000.csv')\n",
    "    metadata = pd.read_csv('/Users/gguillau/Desktop/Practicum/Cuetessa Project/archive/metadata_1_2000.csv')\n",
    "except:\n",
    "    annotations = pd.read_csv(\"datasets/static_annotations_averaged_songs_1_2000.csv\")\n",
    "    metadata = pd.read_csv('datasets/metadata_1_2000.csv')\n",
    "\n",
    "print('Data has been read correctly!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135b8c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('General information about the contract dataset')\n",
    "get_info(annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa2f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce36e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column names contain empty spaces, fix that\n",
    "annotations.columns = [col.replace(\" \",\"\") for col in annotations.columns]\n",
    "# drop extra columns\n",
    "annotations.drop(columns=[\"valence_std\",\"arousal_std\"], inplace=True)\n",
    "# shortent the column names\n",
    "annotations.rename(columns={\"valence_mean\":\"valence\", \"arousal_mean\":\"arousal\"}, inplace=True)\n",
    "annotations.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show general info\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8df8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('General information about the contract dataset')\n",
    "get_info(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beafec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a sample of the meta set\n",
    "metadata.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc688c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy up the column names\n",
    "metadata.columns = [\"song_id\",\"file_name\",\"artist\",\"song_title\",\"segment_start\",\"segment_end\", \"genre\"]\n",
    "# fill in missing song titles\n",
    "metadata[\"song_title\"].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# remove artifacts\n",
    "for col in [\"artist\",\"song_title\",\"genre\"]:\n",
    "    metadata[col] = [re.sub(r\"\\t\", \"\", string) for string in metadata[col]]\n",
    "\n",
    "# constract new file_name column\n",
    "metadata[\"file_name\"] = metadata[\"song_id\"].astype(str) + \".mp3\"\n",
    "\n",
    "# print sample to check\n",
    "metadata.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999beba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct new file_name column and write file path\n",
    "metadata[\"file_name\"] = metadata[\"song_id\"].astype(str) + \".mp3\"\n",
    "metadata[\"file_path\"] = \"/Users/gguillau/Desktop/Practicum/Cuetessa Project/archive/DEAM_audio/MEMD_audio/\" \n",
    "+ metadata[\"file_name\"]\n",
    "\n",
    "metadata.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10783037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "df = annotations.merge(metadata, on=\"song_id\",how=\"outer\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff5fdc",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the valence and arousal values distribution\n",
    "plt.scatter(df[\"valence\"], df[\"arousal\"], alpha=.3)\n",
    "plt.title(\"Valence vs arousal scatter plot\", fontsize=14)\n",
    "plt.xlim([1,9])\n",
    "plt.ylim([1,9])\n",
    "plt.xlabel(\"Valence\")\n",
    "plt.ylabel(\"Arousal\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e211e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of genres present in the dataset\n",
    "nunique_genres = df[\"genre\"].nunique()\n",
    "print(\"Unique genres in the dataset:  {}\\n\".format(nunique_genres))\n",
    "\n",
    "# select the top 10\n",
    "top10_genres = df[\"genre\"].value_counts()[:10]\n",
    "top10_genres.loc[\"Other\"] = df[\"genre\"].value_counts()[10:].sum()\n",
    "\n",
    "# plot a pie chart with genres ratio\n",
    "plt.figure(figsize=(6,6))\n",
    "labels = top10_genres.index\n",
    "plt.pie(top10_genres, labels=labels)\n",
    "plt.title(\"Dataset Song Genres\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf18ae",
   "metadata": {},
   "source": [
    "## Audio Features\n",
    "Investigate the audio features available with the librosa library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150c4c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_idx = np.random.randint(0, 1744)\n",
    "\n",
    "# select a random song from the dataset\n",
    "song = df.loc[random_idx, :]\n",
    "\n",
    "# load the file and print its sampling rate \n",
    "file_path = \"/Users/gguillau/Desktop/Practicum/Cuetessa Project/archive/DEAM_audio/MEMD_audio/\" + song[\"file_name\"]\n",
    "audio, sample_rate = librosa.load(file_path)\n",
    "# print info about this song\n",
    "print(f\"Sampling rate: {sample_rate}\")\n",
    "print(song)\n",
    "\n",
    "# plot the wavefrom\n",
    "plt.figure(figsize=(12,4))\n",
    "librosa.display.waveshow(audio, sr=sample_rate) # plot a waveform and play the file\n",
    "plt.title(f'\"{song.song_title[:15]}\" by {song.artist}, {song.genre}')\n",
    "plt.legend([f\"song id {song.song_id}\"])\n",
    "plt.xlabel(\"Duration in seconds\")\n",
    "\n",
    "# output the audio\n",
    "ipd.Audio(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce928dfb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972c701",
   "metadata": {},
   "source": [
    "### Spectral Centroid\n",
    "\n",
    "The spectral centroid is a measure of the center of gravity of a sound. This can be used to classify the timbre of a sound, and also to identify different types of sounds. It can be used in applications such as Notion AI, where it can be used to identify particular sounds in a recording.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the feature from the audio file\n",
    "centroid = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)\n",
    "\n",
    "# obtain other data for plotting\n",
    "S, phase = librosa.magphase(librosa.stft(y=audio))\n",
    "times = librosa.times_like(centroid)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(title=f\"Spectral Centroid for song id {song.song_id}\")\n",
    "\n",
    "# show spectrogram\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n",
    "                         y_axis=\"log\", x_axis=\"time\", ax=ax)\n",
    "# plot spectral centroid\n",
    "ax.plot(times, centroid.T, label=\"Spectral centroid\",  color=\"w\")\n",
    "ax.legend(loc=\"upper right\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc61f4",
   "metadata": {},
   "source": [
    "### Spectral Rolloff\n",
    "\n",
    "Spectral rolloff is a feature of audio signal analysis which measures how quickly the power of a signal decreases as the frequency increases. It can be used to identify the tonal components of a signal and detect the presence of harmonic content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20789640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate minimum frequencies with roll_percent=0.95\n",
    "rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.95)\n",
    "# approximate minimum frequencies with roll_percent=0.50\n",
    "rolloff_middle = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.5)\n",
    "# approximate minimum frequencies with roll_percent=0.01\n",
    "rolloff_min = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.01)\n",
    "\n",
    "S, phase = librosa.magphase(librosa.stft(audio))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(title=f\"Spectral Roll-off for song id {song.song_id}\")\n",
    "\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n",
    "                         y_axis=\"log\", x_axis=\"time\", ax=ax)\n",
    "\n",
    "ax.plot(librosa.times_like(rolloff), rolloff[0], label=\"Roll-off freq 0.95\", color=\"c\")\n",
    "ax.plot(librosa.times_like(rolloff), rolloff_middle[0], label=\"Roll-off freq 0.50\", color=\"w\")\n",
    "ax.plot(librosa.times_like(rolloff), rolloff_min[0], label=\"Roll-off freq 0.01\", color=\"y\")\n",
    "ax.legend(loc='center right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d7d5b",
   "metadata": {},
   "source": [
    "### Spectral Contrast\n",
    "Spectral contrast, as implemented in the librosa library, is used to highlight the differences in frequency content of a signal, and can be used to detect and identify individual instruments, as well as providing general information about the signal. The process of spectral contrast works by measuring the overall average energy of each frequency band, then subtracting this average from the original signal. This reveals the differences in the signal which would otherwise be difficult to detect. By doing this, spectral contrast can provide an understanding of the structure of a sound, and can even be used to detect and identify individual instruments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.abs(librosa.stft(y=audio))\n",
    "contrast = librosa.feature.spectral_contrast(S=S, sr=sample_rate, n_bands=6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot spectral contrast\n",
    "img2 = librosa.display.specshow(contrast, x_axis=\"time\", ax=ax)\n",
    "fig.colorbar(img2, ax=ax)\n",
    "ax.set(ylabel=\"Frequency bands\", title=f\"Spectral contrast for song id {song.song_id}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea09f0",
   "metadata": {},
   "source": [
    "### Mel-Frequency Cepstrum Coefficients\n",
    "MEL-Frequency Cepstrum Coefficients (MFCCs) are a powerful technique for recognizing patterns in audio signals. They are used to represent audio signals in a form that is more easily analyzed by a machine. MFCCs are derived from a Fourier transform of a signal and are used in speech recognition and music recognition applications. MFCCs are also used in audio fingerprinting and speaker recognition.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f9721",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate mfccs from the audio file\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=24)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "img = librosa.display.specshow(mfccs, x_axis='time', ax=ax)\n",
    "fig.colorbar(img, ax=ax)\n",
    "ax.set(title=f\"Mel-Frequency Cepstrum Coefficients for song id {song.song_id}\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b7365",
   "metadata": {},
   "source": [
    "### Onset Strength\n",
    "The onset strength is used to measure the intensity of different sounds in audio files. It is an important measure to consider when analyzing audio signals, as it can help determine the loudness and intensity of a sound in comparison to other sounds. The OS algorithm is also used to identify and differentiate between different types of sounds, making it particularly useful for audio classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate note onset events by picking peaks in an onset strength envelope\n",
    "# select only a slice for better visualisation\n",
    "onset_env = librosa.onset.onset_strength(y=audio, sr=sample_rate)[750:1000]\n",
    "onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sample_rate)\n",
    "times = librosa.times_like(onset_env, sr=sample_rate)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(title=f\"The Onset Strength for the song id {song.song_id}\", xlabel=\"Time\")\n",
    "\n",
    "ax.plot(times, onset_env, label='Onset strength')\n",
    "ax.vlines(times[onset_frames], 0, onset_env.max(), color='r', alpha=0.3, linestyle='--', label='Onsets')\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a684bc",
   "metadata": {},
   "source": [
    "### Zero Crossing Rate\n",
    "The Zero Crossing Rate (ZCR) is a measure used in signal processing to identify the number of times a signal crosses the zero axis. It is often used to analyze audio signals, as it can provide an indication of the perceptual loudness of the audio. ZCR can be calculated by counting the number of times the signal crosses the zero axis in a given time interval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "zcrs = librosa.feature.zero_crossing_rate(audio)\n",
    "\n",
    "plt.plot(zcrs[0])\n",
    "plt.title(f\"Zero Crossing Rates for song id {song.song_id}\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f0be7",
   "metadata": {},
   "source": [
    "### Chroma Energy Normalized Statistics\n",
    "CENS is calculated by first computing the chroma vectors for each audio signal. The chroma vector is a 12-dimensional vector composed of the energy in each of the 12 semitones of an octave. The chroma vectors of each signal are then normalized to have unit energy. Finally, the normalized chroma vectors are compared using a similarity measure such as cosine distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_cens = librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(chroma_cens, y_axis=\"chroma\", x_axis=\"time\")\n",
    "fig.colorbar(img)\n",
    "ax.set(title=f\"Chroma CENS for song id {song.song_id}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56279be",
   "metadata": {},
   "source": [
    "### Beats Per Minute: Dynamic Tempo\n",
    "The tempo of a song can be defined as its speed, or how quickly the music moves. It can range from slow and gentle to fast and intense, depending on the genre and style of the music. In a dynamic tempo, the speed of the music changes throughout the song, creating a sense of excitement and anticipation. This can be done by increasing the tempo gradually or by sudden changes in speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_env = librosa.onset.onset_strength(y=audio, sr=sample_rate)\n",
    "dtempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sample_rate, aggregate=None)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tg = librosa.feature.tempogram(onset_envelope=onset_env, sr=sample_rate, hop_length=512)\n",
    "librosa.display.specshow(tg, x_axis=\"time\", y_axis=\"tempo\", cmap=\"magma\", ax=ax)\n",
    "\n",
    "ax.plot(librosa.times_like(dtempo), dtempo, color=\"w\", linewidth=1.5, label=\"Tempo estimate\")\n",
    "\n",
    "ax.set(title=f\"Dynamic tempo estimation for song id {song.song_id}\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c097561",
   "metadata": {},
   "source": [
    "### Root Mean Square\n",
    "RMS is a measure of the average power of a signal over time. It is useful for comparing the loudness of different audio signals, as well as for analyzing the spectral content of signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rms = librosa.feature.rms(y=audio)[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "times = librosa.times_like(rms)\n",
    "ax.set(title=f\"RMS energy for each frame for song id {song.song_id}\", xlabel=\"Time\")\n",
    "ax.semilogy(times, rms, label='RMS Energy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd69cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = librosa.feature.rms(y=audio)[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "times = librosa.times_like(rms)\n",
    "ax.set(title=f\"RMS energy for each frame for song id {song.song_id}\", xlabel=\"Time\")\n",
    "ax.semilogy(times, rms, label='RMS Energy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essentia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c2210",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(array):\n",
    "    \"\"\"\n",
    "    Takes an array and gives back its mean, standard deviation, \n",
    "    first-order difference mean, and first-order difference \n",
    "    standard deviation — in this exact order.\n",
    "    \"\"\"\n",
    "    mean = array.mean()\n",
    "    var = array.var()\n",
    "    \n",
    "    diff_mean = np.diff(array).mean()\n",
    "    diff_var = np.diff(array).var()\n",
    "    \n",
    "    return [mean, var, diff_mean, diff_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0509553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    \"\"\"\n",
    "    Takes path to an audio file and returns statistics (mean, standard deviation, \n",
    "    first-order difference mean, and first-order difference standard deviation)\n",
    "    for each output array of the following methods:\n",
    "    \n",
    "    - Spectral Centroid (1)\n",
    "    - Spectral Rolloff (3)\n",
    "    - Spectral Contrast (7)\n",
    "    - MFCC (24)\n",
    "    – Onset Strength (1)\n",
    "    – Zero Crossing Rate (1)\n",
    "    – CENS (12)\n",
    "    – BPM Dynamic (1)\n",
    "    – RMS (1)\n",
    "    \n",
    "      204 values output in total.\n",
    "      \n",
    "    \"\"\"\n",
    "    features = [] # empty list for storing features\n",
    "    cnt = 0 # counter for keeping track of features number\n",
    "    \n",
    "    # load the audio file \n",
    "    audio, sample_rate = librosa.load(file_path)\n",
    "    \n",
    "    # Spectral Centroid\n",
    "    cent = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)\n",
    "    features.append(get_stats(cent))\n",
    "    cnt += 1\n",
    "    \n",
    "    # Spectral Rolloff\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.95)\n",
    "    features.append(get_stats(rolloff))\n",
    "    cnt += 1\n",
    "    \n",
    "    rolloff_middle = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.5)\n",
    "    features.append(get_stats(rolloff_middle))\n",
    "    cnt += 1\n",
    "    \n",
    "    rolloff_min = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate, roll_percent=0.01)\n",
    "    features.append(get_stats(rolloff_min))\n",
    "    cnt += 1\n",
    "    \n",
    "    # Spectral Contrast\n",
    "    S = np.abs(librosa.stft(y=audio))\n",
    "    contrast = librosa.feature.spectral_contrast(S=S, sr=sample_rate)\n",
    "    for band in contrast:\n",
    "        features.append(get_stats(band))\n",
    "        cnt += 1\n",
    "        \n",
    "    # MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=24)\n",
    "    for mfcc in mfccs:\n",
    "        features.append(get_stats(mfcc))\n",
    "        cnt += 1\n",
    "    \n",
    "    # Onset strength\n",
    "    onset_env = librosa.onset.onset_strength(y=audio, sr=sample_rate)\n",
    "    features.append(get_stats(onset_env))\n",
    "    cnt += 1\n",
    "    \n",
    "    # ZCR\n",
    "    zcrs = librosa.feature.zero_crossing_rate(audio)\n",
    "    features.append(get_stats(zcrs))\n",
    "    cnt += 1\n",
    "    \n",
    "    # CENS\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
    "    for chroma in chroma_cens:\n",
    "        features.append(get_stats(chroma))\n",
    "        cnt += 1\n",
    "        \n",
    "    # BPM\n",
    "    dtempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sample_rate, aggregate=None)\n",
    "    features.append(get_stats(dtempo))\n",
    "    cnt += 1\n",
    "    \n",
    "    # RMS\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    features.append(get_stats(rms))\n",
    "    cnt += 1\n",
    "    \n",
    "    features = np.array(features).reshape(cnt * 4)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e702243",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    \n",
    "     file_path = df.loc[i, \"file_path\"]\n",
    "     features_df.append(extract_features(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(np.array(features_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9116d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df.to_csv(\"features_df1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv(\"/Users/gguillau/Desktop/Practicum/Cuetessa Project/features_df1.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a featuers amount lookup table\n",
    "lkp_dict = dict(centroid=1, rolloff_high=1, rolloff_mid=1, rolloff_min=1, \n",
    "              contrast=7, mfcc=24, onset=1, zcr=1, cens=12, bpm=1, rms=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "statistics = [\"mean\",\"var\",\"diff_mean\",\"diff_var\"]\n",
    "\n",
    "for feature in lkp_dict:\n",
    "    for i in range(1, lkp_dict[feature] + 1, 1):\n",
    "        for statistic in statistics:\n",
    "            if lkp_dict[feature] != 1:\n",
    "                column_names.append(feature + \"_\" + str(i) + \"_\" + statistic)\n",
    "            else:\n",
    "                column_names.append(feature + \"_\" + statistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(np.array(features_df), columns=column_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_df.values\n",
    "y = df.valence.values\n",
    "\n",
    "# split the data into the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creata a function for models evaluation\n",
    "def eval_model(model, X=X_train, y=y_train, show_metrics=1):\n",
    "    \n",
    "    # create a pipline to avoid possible target leakage\n",
    "    pipe = make_pipeline(StandardScaler(), model)\n",
    "    \n",
    "    scores = cross_validate(pipe, X, y, cv=5, scoring=(\"r2\", \"neg_mean_absolute_error\"), n_jobs=-1)\n",
    "    \n",
    "    r2 = np.average(scores[\"test_r2\"])\n",
    "    mae = abs(np.average(scores[\"test_neg_mean_absolute_error\"]))\n",
    "    fit_time = np.average(scores[\"fit_time\"])\n",
    "    score_time = np.average(scores[\"score_time\"])\n",
    "    \n",
    "    if show_metrics == 1:\n",
    "        print(\"Fit time: {:.5f}\".format(fit_time))\n",
    "        print(\"Score time: {:.5f}\".format(score_time))\n",
    "        print(\"R2: {:.4f}\".format(r2))\n",
    "        print(\"MAE {:.4f}\".format(mae))\n",
    "    else:\n",
    "        return r2, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(SVR())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68554bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(KNeighborsRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(LinearRegression())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
